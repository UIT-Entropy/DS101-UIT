{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/ck-aio-hutech (1).zip -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJLSP2g89H3o",
        "outputId": "ea850583-c4de-43a6-9789-26093d6c4f38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `unzip /content/ck-aio-hutech (1).zip -d /content/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### START: CÁC KHAI BÁO CHÍNH - KHÔNG THAY ĐỔI ###\n",
        "SEED = 46  # Số seed (Ban tổ chức sẽ công bố & thay đổi vào lúc chấm)\n",
        "TRAIN_DATA_DIR_PATH = '/content/train2'\n",
        "TEST_DATA_DIR_PATH = '/content/train2/tuyết khô'\n",
        "PATH_OAI086 = \"/content\"\n",
        "Submission_path = \"/content/sample_data\"\n",
        "### END: CÁC KHAI BÁO CHÍNH - KHÔNG THAY ĐỔI ###\n",
        "\n",
        "### START: CÁC THƯ VIỆN IMPORT ###\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from torchvision.io import read_image\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.models import ResNet101_Weights\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import gc\n",
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "### END: CÁC THƯ VIỆN IMPORT ###\n",
        "\n",
        "### START: SEEDING EVERYTHING - KHÔNG THAY ĐỔI ###\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "### END: SEEDING EVERYTHING - KHÔNG THAY ĐỔI ###\n",
        "\n",
        "### START: ĐỊNH NGHĨA & CHẠY HUẤN LUYỆN MÔ HÌNH ###\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 10\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device with {torch.cuda.device_count()} GPUs\")\n",
        "\n",
        "# Hàm chuẩn hóa tên thư mục\n",
        "def normalize_folder_name(name):\n",
        "    name = ''.join(c for c in unicodedata.normalize('NFD', name) if unicodedata.category(c) != 'Mn')\n",
        "    name = name.replace('+', ' ').strip()\n",
        "    return name.lower()\n",
        "\n",
        "# Transform cho Grayscale\n",
        "train_transform_grayscale = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: TF.rgb_to_grayscale(x)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),\n",
        "    transforms.Lambda(lambda x: F.interpolate(x.unsqueeze(0), size=IMG_SIZE, mode='bilinear', align_corners=False).squeeze(0)),\n",
        "    transforms.Lambda(lambda x: x / 255.0),\n",
        "])\n",
        "\n",
        "test_transform_grayscale = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: TF.rgb_to_grayscale(x)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),\n",
        "    transforms.Lambda(lambda x: F.interpolate(x.unsqueeze(0), size=IMG_SIZE, mode='bilinear', align_corners=False).squeeze(0)),\n",
        "    transforms.Lambda(lambda x: x / 255.0),\n",
        "])\n",
        "\n",
        "# Transform cho RGB\n",
        "train_transform_rgb = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: F.interpolate(x.unsqueeze(0), size=IMG_SIZE, mode='bilinear', align_corners=False).squeeze(0)),\n",
        "    transforms.Lambda(lambda x: x / 255.0),\n",
        "])\n",
        "\n",
        "test_transform_rgb = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: F.interpolate(x.unsqueeze(0), size=IMG_SIZE, mode='bilinear', align_corners=False).squeeze(0)),\n",
        "    transforms.Lambda(lambda x: x / 255.0),\n",
        "])\n",
        "\n",
        "# Dataset tùy chỉnh cho tập huấn luyện\n",
        "class CustomTrainDataset(Dataset):\n",
        "    def __init__(self, root, transform=None, augmentations=None, augment_classes=None, include_classes=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.classes = sorted(os.listdir(root))\n",
        "        print(\"Danh sách thư mục thực tế:\", self.classes)\n",
        "\n",
        "        # Định nghĩa ánh xạ lớp\n",
        "        self.class_to_idx = {\n",
        "            normalize_folder_name('dong co'): 0,\n",
        "            normalize_folder_name('tai meo'): 1,\n",
        "            normalize_folder_name('tuyet kho'): 2,\n",
        "        }\n",
        "        print(\"Class to index mapping:\", self.class_to_idx)\n",
        "        # Định nghĩa ánh xạ lớp\n",
        "        self.class_to_idx = {\n",
        "            normalize_folder_name('dong co'): 0,\n",
        "            normalize_folder_name('tai meo'): 1,\n",
        "            normalize_folder_name('tuyet kho'): 2,\n",
        "            normalize_folder_name('đông cô'): 0, # Added mapping for 'đông cô'\n",
        "        }\n",
        "        # Kiểm tra xem các lớp thực tế có khớp với class_to_idx không\n",
        "        for cls in self.classes:\n",
        "            normalized_cls = normalize_folder_name(cls)\n",
        "            if normalized_cls not in self.class_to_idx:\n",
        "                raise ValueError(f\"Thư mục '{cls}' không được ánh xạ trong class_to_idx.\")\n",
        "\n",
        "        self.augment_class_indices = (\n",
        "            [self.class_to_idx[cls] for cls in augment_classes]\n",
        "            if augment_classes is not None else []\n",
        "        )\n",
        "        self.include_classes = include_classes if include_classes is not None else list(self.class_to_idx.values())\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        for cls in self.classes:\n",
        "            cls_idx = self.class_to_idx[normalize_folder_name(cls)]\n",
        "            if cls_idx in self.include_classes:\n",
        "                cls_path = os.path.join(root, cls)\n",
        "                for img_name in os.listdir(cls_path):\n",
        "                    if img_name.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        self.images.append(os.path.join(cls_path, img_name))\n",
        "                        # Ánh xạ nhãn cho Grayscale hoặc RGB\n",
        "                        if self.include_classes == [0, 1]:\n",
        "                            self.labels.append(0 if cls_idx == 0 else 1)\n",
        "                        else:\n",
        "                            self.labels.append(cls_idx)\n",
        "        print(\"Labels created:\", set(self.labels))\n",
        "\n",
        "        if not self.images:\n",
        "            raise ValueError(\"Không tìm thấy hình ảnh nào trong các thư mục được chỉ định.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        total = 0\n",
        "        for label in self.labels:\n",
        "            if label in self.augment_class_indices:\n",
        "                total += 1 + (len(self.augmentations) if self.augmentations else 0)\n",
        "            else:\n",
        "                total += 1\n",
        "        return total\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        current_idx = 0\n",
        "        for i, label in enumerate(self.labels):\n",
        "            num_versions = 1 + (len(self.augmentations) if label in self.augment_class_indices and self.augmentations else 0)\n",
        "            if idx < current_idx + num_versions:\n",
        "                img_idx = i\n",
        "                aug_idx = idx - current_idx\n",
        "                break\n",
        "            current_idx += num_versions\n",
        "\n",
        "        img_path = self.images[img_idx]\n",
        "        label = self.labels[img_idx]\n",
        "        image = read_image(img_path).float()\n",
        "\n",
        "        if label in self.augment_class_indices and self.augmentations and aug_idx > 0:\n",
        "            aug_transform = self.augmentations[aug_idx - 1]\n",
        "            image = aug_transform(image)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Dataset cho tập test\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = sorted(\n",
        "            [f for f in os.listdir(root_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n",
        "            key=lambda x: int(re.sub('\\D', '', x))\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.images[idx])\n",
        "        image = read_image(img_path).float()\n",
        "        file_name = self.images[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, file_name\n",
        "\n",
        "# Augmentations với grayscale\n",
        "augmentations_grayscale = [\n",
        "    transforms.Compose([\n",
        "        transforms.Lambda(lambda x: TF.rgb_to_grayscale(x)),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),\n",
        "        transforms.Lambda(lambda x: F.interpolate(x.unsqueeze(0), size=IMG_SIZE, mode='bilinear', align_corners=False).squeeze(0)),\n",
        "        transforms.RandomHorizontalFlip(p=1),\n",
        "        transforms.Lambda(lambda x: x / 255.0),\n",
        "    ]),\n",
        "    transforms.Compose([\n",
        "        transforms.Lambda(lambda x: TF.rgb_to_grayscale(x)),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),\n",
        "        transforms.Lambda(lambda x: F.interpolate(x.unsqueeze(0), size=IMG_SIZE, mode='bilinear', align_corners=False).squeeze(0)),\n",
        "        transforms.RandomVerticalFlip(p=1),\n",
        "        transforms.Lambda(lambda x: x / 255.0),\n",
        "    ]),\n",
        "]\n",
        "\n",
        "# Augmentations với RGB\n",
        "augmentations_rgb = [\n",
        "    transforms.Compose([\n",
        "        transforms.Lambda(lambda x: F.interpolate(x.unsqueeze(0), size=IMG_SIZE, mode='bilinear', align_corners=False).squeeze(0)),\n",
        "        transforms.RandomHorizontalFlip(p=1),\n",
        "        transforms.Lambda(lambda x: x / 255.0),\n",
        "    ]),\n",
        "    transforms.Compose([\n",
        "        transforms.Lambda(lambda x: F.interpolate(x.unsqueeze(0), size=IMG_SIZE, mode='bilinear', align_corners=False).squeeze(0)),\n",
        "        transforms.RandomVerticalFlip(p=1),\n",
        "        transforms.Lambda(lambda x: x / 255.0),\n",
        "    ]),\n",
        "]\n",
        "\n",
        "# Tạo dataset và dataloader\n",
        "Train_ds_grayscale = CustomTrainDataset(\n",
        "    root=TRAIN_DATA_DIR_PATH,\n",
        "    transform=train_transform_grayscale,\n",
        "    include_classes=[0, 1]\n",
        ")\n",
        "Train_loader_grayscale = DataLoader(Train_ds_grayscale, batch_size=BATCH_SIZE, shuffle=True)\n",
        "Test_ds_grayscale = TestDataset(root_dir=TEST_DATA_DIR_PATH, transform=test_transform_grayscale)\n",
        "Test_loader_grayscale = DataLoader(Test_ds_grayscale, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "Train_ds_rgb = CustomTrainDataset(\n",
        "    root=TRAIN_DATA_DIR_PATH,\n",
        "    transform=train_transform_rgb,\n",
        "    include_classes=[0, 1, 2]\n",
        ")\n",
        "Train_loader_rgb = DataLoader(Train_ds_rgb, batch_size=BATCH_SIZE, shuffle=True)\n",
        "Test_ds_rgb = TestDataset(root_dir=TEST_DATA_DIR_PATH, transform=test_transform_rgb)\n",
        "Test_loader_rgb = DataLoader(Test_ds_rgb, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "test_filename = [file_name for _, file_name in Test_ds_grayscale]\n",
        "\n",
        "print(f\"Số mẫu trong Train_ds_grayscale (lớp 0,1): {len(Train_ds_grayscale)}\")\n",
        "print(f\"Số lớp trong Train_ds_grayscale: {len(set(Train_ds_grayscale.labels))}\")\n",
        "print(f\"Số mẫu trong Train_ds_rgb (lớp 0,1,2): {len(Train_ds_rgb)}\")\n",
        "print(f\"Số lớp trong Train_ds_rgb: {len(set(Train_ds_rgb.labels))}\")\n",
        "\n",
        "# Các lớp mô hình CBAM và ResNetCBAM\n",
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
        "        super(BasicConv, self).__init__()\n",
        "        self.out_channels = out_planes\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
        "        self.relu = nn.ReLU() if relu else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class ChannelGate(nn.Module):\n",
        "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
        "        super(ChannelGate, self).__init__()\n",
        "        self.gate_channels = gate_channels\n",
        "        self.mlp = nn.Sequential(\n",
        "            Flatten(),\n",
        "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
        "        )\n",
        "        self.pool_types = pool_types\n",
        "\n",
        "    def forward(self, x):\n",
        "        channel_att_sum = None\n",
        "        for pool_type in self.pool_types:\n",
        "            if pool_type == 'avg':\n",
        "                avg_pool = F.avg_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp(avg_pool)\n",
        "            elif pool_type == 'max':\n",
        "                max_pool = F.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp(max_pool)\n",
        "            elif pool_type == 'lp':\n",
        "                lp_pool = F.lp_pool2d(x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp(lp_pool)\n",
        "            elif pool_type == 'lse':\n",
        "                lse_pool = logsumexp_2d(x)\n",
        "                channel_att_raw = self.mlp(lse_pool)\n",
        "\n",
        "            if channel_att_sum is None:\n",
        "                channel_att_sum = channel_att_raw\n",
        "            else:\n",
        "                channel_att_sum = channel_att_sum + channel_att_raw\n",
        "\n",
        "        scale = F.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
        "        return x * scale\n",
        "\n",
        "def logsumexp_2d(tensor):\n",
        "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
        "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
        "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
        "    return outputs\n",
        "\n",
        "class ChannelPool(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.cat((torch.max(x, 1)[0].unsqueeze(1), torch.mean(x, 1).unsqueeze(1)), dim=1)\n",
        "\n",
        "class SpatialGate(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialGate, self).__init__()\n",
        "        kernel_size = 7\n",
        "        self.compress = ChannelPool()\n",
        "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_compress = self.compress(x)\n",
        "        x_out = self.spatial(x_compress)\n",
        "        scale = F.sigmoid(x_out)\n",
        "        return x * scale\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False, no_channel=False):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.no_channel = no_channel\n",
        "        if not no_channel:\n",
        "            self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
        "        self.no_spatial = no_spatial\n",
        "        if not no_spatial:\n",
        "            self.SpatialGate = SpatialGate()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_out = x\n",
        "        if not self.no_channel:\n",
        "            x_out = self.ChannelGate(x_out)\n",
        "        if not self.no_spatial:\n",
        "            x_out = self.SpatialGate(x_out)\n",
        "        return x_out\n",
        "\n",
        "class ResNetCBAM(nn.Module):\n",
        "    def __init__(self, num_classes=4, weights=ResNet101_Weights.IMAGENET1K_V1, no_channel=False):\n",
        "        super(ResNetCBAM, self).__init__()\n",
        "        self.resnet = models.resnet101(weights=weights)\n",
        "        self.cbam1 = CBAM(256, no_channel=no_channel)\n",
        "        self.cbam2 = CBAM(512, no_channel=no_channel)\n",
        "        self.cbam3 = CBAM(1024, no_channel=no_channel)\n",
        "        self.cbam4 = CBAM(2048, no_channel=no_channel)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet.conv1(x)\n",
        "        x = self.resnet.bn1(x)\n",
        "        x = self.resnet.relu(x)\n",
        "        x = self.resnet.maxpool(x)\n",
        "        x = self.resnet.layer1(x)\n",
        "        x = self.cbam1(x)\n",
        "        x = self.resnet.layer2(x)\n",
        "        x = self.cbam2(x)\n",
        "        x = self.resnet.layer3(x)\n",
        "        x = self.cbam3(x)\n",
        "        x = self.resnet.layer4(x)\n",
        "        x = self.cbam4(x)\n",
        "        x = self.resnet.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.resnet.fc(x)\n",
        "        return x\n",
        "\n",
        "class ResNetCBAM2(nn.Module):\n",
        "    def __init__(self, num_classes=4, weights=ResNet101_Weights.IMAGENET1K_V1):\n",
        "        super(ResNetCBAM2, self).__init__()\n",
        "        self.resnet = models.resnet101(weights=weights)\n",
        "        self.cbam1 = CBAM(256)\n",
        "        self.cbam2 = CBAM(512)\n",
        "        self.cbam3 = CBAM(1024)\n",
        "        self.cbam4 = CBAM(2048)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet.conv1(x)\n",
        "        x = self.resnet.bn1(x)\n",
        "        x = self.resnet.relu(x)\n",
        "        x = self.resnet.maxpool(x)\n",
        "        x = self.resnet.layer1(x)\n",
        "        x = self.cbam1(x)\n",
        "        x = self.resnet.layer2(x)\n",
        "        x = self.resnet.layer3(x)\n",
        "        x = self.resnet.layer4(x)\n",
        "        x = self.cbam4(x)\n",
        "        x = self.resnet.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.resnet.fc(x)\n",
        "        return x\n",
        "\n",
        "# Hàm mất mát LabelSmoothingLoss\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, smoothing=0.1, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
        "\n",
        "# Hàm mất mát FocalLoss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        return focal_loss\n",
        "\n",
        "# Hàm huấn luyện\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    total_loss, correct = 0, 0\n",
        "    for X, y in dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct / size\n",
        "    print(f\"Train Loss: {avg_loss:.6f}, Accuracy: {accuracy:.4f}\")\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Hàm test với xác suất\n",
        "def test_with_probs(dataloader, model, is_grayscale):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, _ in dataloader:\n",
        "            X = X.to(device)\n",
        "            pred = model(X)\n",
        "            probs = F.softmax(pred, dim=1)\n",
        "            pred_classes = pred.argmax(1)\n",
        "            predictions.extend(pred_classes.cpu().numpy())\n",
        "            probabilities.extend(probs.cpu().numpy())\n",
        "\n",
        "    return predictions, probabilities\n",
        "\n",
        "# Giai đoạn 1: Mô hình Grayscale (chỉ lớp 0,1, num_classes=2)\n",
        "print(\"Phase 1:\")\n",
        "model_grayscale = ResNetCBAM(num_classes=2, no_channel=True)\n",
        "for param in model_grayscale.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model_grayscale.resnet.layer2.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model_grayscale.resnet.layer3.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model_grayscale.resnet.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model_grayscale.resnet.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model_grayscale = model_grayscale.to(device)\n",
        "\n",
        "optimizer_grayscale = torch.optim.Adam(filter(lambda p: p.requires_grad, model_grayscale.parameters()), lr=0.00005)\n",
        "class_weights_grayscale = torch.tensor([1.0, 1.0], dtype=torch.float).to(device)\n",
        "loss_fn_grayscale = LabelSmoothingLoss(classes=2, smoothing=0.05)\n",
        "scheduler_grayscale = torch.optim.lr_scheduler.StepLR(optimizer_grayscale, step_size=5, gamma=0.5)\n",
        "\n",
        "epochs_grayscale = 15\n",
        "for t in range(epochs_grayscale):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_accuracy = train(Train_loader_grayscale, model_grayscale, loss_fn_grayscale, optimizer_grayscale)\n",
        "    if t < epochs_grayscale - 1:\n",
        "        scheduler_grayscale.step()\n",
        "\n",
        "# Giai đoạn 2: Mô hình RGB (tất cả lớp 0,1,2, num_classes=3)\n",
        "print(\"\\nPhase 2:\")\n",
        "model_rgb = ResNetCBAM2(num_classes=3)\n",
        "for param in model_rgb.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model_rgb.resnet.layer2.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model_rgb.resnet.layer3.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model_rgb.resnet.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model_rgb.resnet.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model_rgb = model_rgb.to(device)\n",
        "\n",
        "optimizer_rgb = torch.optim.Adam(filter(lambda p: p.requires_grad, model_rgb.parameters()), lr=0.00005)\n",
        "class_weights_rgb = torch.tensor([1.0, 1.0, 2.5], dtype=torch.float).to(device)\n",
        "loss_fn_rgb = FocalLoss(gamma=2.5, alpha=class_weights_rgb)\n",
        "scheduler_rgb = torch.optim.lr_scheduler.StepLR(optimizer_rgb, step_size=5, gamma=0.5)\n",
        "\n",
        "epochs_rgb = 10\n",
        "for t in range(epochs_rgb):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_accuracy = train(Train_loader_rgb, model_rgb, loss_fn_rgb, optimizer_rgb)\n",
        "    if t < epochs_rgb - 1:\n",
        "        scheduler_rgb.step()\n",
        "\n",
        "# Thực hiện test với xác suất\n",
        "pred_grayscale, probs_grayscale = test_with_probs(Test_loader_grayscale, model_grayscale, is_grayscale=True)\n",
        "pred_rgb, probs_rgb = test_with_probs(Test_loader_rgb, model_rgb, is_grayscale=False)\n",
        "\n",
        "# Kết hợp dự đoán dựa trên xác suất\n",
        "print(\"\\nCombining Predictions...\")\n",
        "final_predictions = []\n",
        "for i in range(len(pred_rgb)):\n",
        "    if pred_rgb[i] == 1 and probs_rgb[i][1] > 0.7:  # Ưu tiên RGB cho nhãn 1 nếu xác suất cao\n",
        "        final_predictions.append(pred_rgb[i])\n",
        "    else:\n",
        "        # Sử dụng Grayscale cho nhãn 0 hoặc 1, nếu không thì lấy RGB\n",
        "        if pred_grayscale[i] in [0, 1]:\n",
        "            final_predictions.append(pred_grayscale[i])\n",
        "        else:\n",
        "            final_predictions.append(pred_rgb[i])\n",
        "\n",
        "# Lưu kết quả cuối cùng\n",
        "results_df = pd.DataFrame({'image_name': test_filename, 'label': final_predictions})\n",
        "results_df.to_csv(os.path.join(Submission_path, 'results.csv'), index=False)\n",
        "print(\"FINISH\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5CSaQaiqWqD",
        "outputId": "2cfcfc96-cefb-4bdb-8c72-f3479e41ed84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device with 1 GPUs\n",
            "Danh sách thư mục thực tế: ['tai mèo', 'tuyết khô', 'đông cô']\n",
            "Class to index mapping: {'dong co': 0, 'tai meo': 1, 'tuyet kho': 2}\n",
            "Labels created: {0, 1}\n",
            "Danh sách thư mục thực tế: ['tai mèo', 'tuyết khô', 'đông cô']\n",
            "Class to index mapping: {'dong co': 0, 'tai meo': 1, 'tuyet kho': 2}\n",
            "Labels created: {0, 1, 2}\n",
            "Số mẫu trong Train_ds_grayscale (lớp 0,1): 700\n",
            "Số lớp trong Train_ds_grayscale: 2\n",
            "Số mẫu trong Train_ds_rgb (lớp 0,1,2): 1050\n",
            "Số lớp trong Train_ds_rgb: 3\n",
            "Phase 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:00<00:00, 188MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Train Loss: 0.345567, Accuracy: 0.9271\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train Loss: 0.237279, Accuracy: 0.9914\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train Loss: 0.230538, Accuracy: 0.9943\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Train Loss: 0.223587, Accuracy: 0.9971\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Train Loss: 0.213211, Accuracy: 1.0000\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Train Loss: 0.219199, Accuracy: 0.9929\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Train Loss: 0.215023, Accuracy: 0.9986\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Train Loss: 0.212551, Accuracy: 0.9986\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Train Loss: 0.209702, Accuracy: 1.0000\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Train Loss: 0.207108, Accuracy: 1.0000\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Train Loss: 0.207865, Accuracy: 0.9986\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Train Loss: 0.205863, Accuracy: 0.9986\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Train Loss: 0.202297, Accuracy: 1.0000\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Train Loss: 0.203147, Accuracy: 1.0000\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Train Loss: 0.202255, Accuracy: 1.0000\n",
            "\n",
            "Phase 2:\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Train Loss: 0.291338, Accuracy: 0.8362\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train Loss: 0.047533, Accuracy: 0.9762\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train Loss: 0.027677, Accuracy: 0.9762\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Train Loss: 0.045120, Accuracy: 0.9705\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Train Loss: 0.046002, Accuracy: 0.9667\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Train Loss: 0.024559, Accuracy: 0.9810\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Train Loss: 0.023720, Accuracy: 0.9876\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Train Loss: 0.012340, Accuracy: 0.9924\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Train Loss: 0.003623, Accuracy: 0.9990\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Train Loss: 0.003658, Accuracy: 0.9990\n",
            "\n",
            "Combining Predictions...\n",
            "FINISH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### END: THỰC NGHIỆM & XUẤT FILE KẾT QUẢ RA CSV ###\n",
        "Submission_path = \"/content/\"\n",
        "\n",
        "\n",
        "# Kết hợp dự đoán\n",
        "print(\"\\nCombining Predictions... \")\n",
        "final_predictions = []\n",
        "for i in range(len(pred_rgb)):\n",
        "    if pred_rgb[i] == 1:\n",
        "        final_predictions.append(pred_rgb[i])\n",
        "    else:\n",
        "        final_predictions.append(pred_grayscale[i])\n",
        "\n",
        "# Lưu kết quả cuối cùng\n",
        "results_df = pd.DataFrame({'image_name': test_filename, 'label': final_predictions})\n",
        "results_df.to_csv(os.path.join(Submission_path,'results.csv'), index=False)\n",
        "print(\"FINISH\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se1iYlZ1eGit",
        "outputId": "49d76b3d-e552-4bd1-e877-9d37ef6f10ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Combining Predictions... \n",
            "FINISH\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}